# 2022/2023 Josue Page Vizcaino pv.josue@gmail.com
import subprocess
import numpy as np
import torch, math, csv
import torch.nn.functional as F
import torch.nn as nn
from typing import Tuple
from tifffile import imsave
from tqdm import tqdm
import matplotlib.pyplot as plt

from XLFMDataset import *

def get_free_gpu():
    """
    This function returns the index of the GPU with the most available memory.
    It does so by running a shell command to get the memory usage of each GPU,
    and then returns the index of the GPU with the least memory usage.
    @return The index of the GPU with the most available memory.
    """
    _, output = subprocess.getstatusoutput(
        "nvidia-smi -q -d Memory |grep -A4 GPU|grep Used"
    )
    memory_available = [int(s) for s in output.split() if s.isdigit()]
    return np.argmin(memory_available)

def get_lenslet_centers(filename):
    """
    Given a filename, read in the data and return the lenslet coordinates.
    @param filename - the name of the file containing the lenslet coordinates
    @return The lenslet coordinates as a tensor.
    """
    x,y = [], []
    with open(filename,'r') as f:
        reader = csv.reader(f,delimiter='\t')
        for row in reader:
            x.append(int(row[0]))
            y.append(int(row[1]))
    lenslet_coords = torch.cat((torch.IntTensor(x).unsqueeze(1),torch.IntTensor(y).unsqueeze(1)),1)
    return lenslet_coords

def _no_grad_trunc_normal_(tensor, mean=0, std=1, a=-1, b=1):
    """
    This function initializes the weights of a tensor using a truncated normal distribution. The function takes in a tensor, mean, standard deviation, lower bound, and upper bound. It then calculates the cumulative distribution function of the normal distribution and uses it to calculate the lower and upper bounds of the truncated normal distribution. The tensor is then initialized with values from the truncated normal distribution, transformed using the inverse error function, and scaled by the standard deviation and mean. Finally, the tensor is clamped to the specified lower and upper bounds.
    @param tensor - the tensor to initialize
    @param mean - the mean of the normal distribution
    @param std - the standard deviation of the normal distribution
    @param a - the lower bound of the truncated normal distribution
    @param b - the upper
    """
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor

def fast_quantile(x, quant=0.95):
    """
    Calculate the quantile of a given input tensor or numpy array.
    @param x - the input tensor or numpy array
    @param quant - the quantile to calculate (default 0.95)
    @return The quantile value
    """
    
    if torch.is_tensor(x):
        h,ranges = torch.histogram(x, bins=10000)
    else:
        h,ranges = np.histogram(x, bins=10000)
    quant_numel = h[1:].sum() * quant
    cumulative_elements = 0
    for n_bin in range(1,len(h)):
        if cumulative_elements >= quant_numel:
            break
        cumulative_elements += h[n_bin]
    return ranges[n_bin]


def crop_volume_center(volume, volume_shape):
    """
    Crop a volume to the center of the volume.
    @param volume - the volume to crop
    @param volume_shape - the shape of the volume
    @return The cropped volume
    """
    vol_pred_crop_width = volume_shape[2]
    vol_pred_crop_height = volume_shape[3]
    vol_pred_center_x = math.floor(volume.shape[2] / 2)
    vol_pred_center_y = math.floor(volume.shape[3] / 2)
    volume = volume[
        :,
        :,
        vol_pred_center_x
        - math.floor(vol_pred_crop_width / 2) : vol_pred_center_x
        + math.ceil(vol_pred_crop_width / 2),
        vol_pred_center_y
        - math.floor(vol_pred_crop_height / 2) : vol_pred_center_y
        + math.ceil(vol_pred_crop_height / 2),
    ]
    return volume

def load_process_volume(data_path, volume_new_size=[], volume_ths=[], norm='max', resize=False, channel_order='zxy', device="cpu"):
    """
    This function loads and processes a volume from a given data path. The volume can be in either .h5 or .tif format. The volume can be resized or cropped to a given size. The volume can also be normalized by either standard deviation or maximum value. 
    @param data_path - the path to the data
    @param volume_new_size - the new size of the volume
    @param volume_ths - the threshold value for the volume
    @param norm - the normalization method to use
    @param resize - whether or not to resize the volume
    @param channel_order - the order of the channels
    @param device - the device to use
    @return the processed volume
    """
    # Check if data_path is a volume or a path
    if isinstance(data_path, str):
        if 'h5' in data_path or 'hdf5' in data_path:
            vol_file = tables.open_file(data_path, "r", driver="H5FD_CORE")
            gt_volume = (
                torch.tensor(vol_file.root.fish_phantom)
            )
            vol_file.close()
        elif 'tif' in data_path:
            gt_volume = read_tiff_stack(data_path, np.float32).to(device)#imread(data_path).squeeze().astype(np.float32)

            # gt_volume = torch.from_numpy(gt_volume).to(device)
        else:
            raise NotImplementedError
    else: # is a tensor
        gt_volume = data_path

    if gt_volume.ndim == 3:
        if channel_order=='xyz':
            gt_volume = gt_volume.permute(2, 1, 0)
        if channel_order=='yxz':
            gt_volume = gt_volume.permute(2, 0, 1)
        gt_volume = gt_volume.unsqueeze(0).to(device)
    
    if resize:
        out_volume = resize_volume(gt_volume.float(), volume_new_size)
    else:
        out_volume = crop_volume_center(gt_volume, [1,volume_new_size[2],volume_new_size[0],volume_new_size[1]])
    
    
    if norm!=None:
        if norm=='std':
            mean,std = torch.std_mean(out_volume)
            out_volume = (out_volume-mean)/std
        elif norm=='max':
            out_volume /= out_volume.max()
            out_volume[out_volume < volume_ths] = 0
    else:
        if isinstance(volume_ths,float):
            out_volume[out_volume.float()<=volume_ths*out_volume.float().max()] = 0
        elif len(volume_ths)==2:
            out_volume[out_volume.float()<volume_ths[0]] = 0
            out_volume[out_volume.float()>=volume_ths[1]] = volume_ths[1]

    return out_volume.type_as(gt_volume)


def load_XLFM_data(dataset_path, lenslet_coords_file, vol_shape, img_shape, images_to_use, n_depths_to_fill, ds_id, volume_ths, volume_quantiles, img_ths, norm):
    """
    Load and process XLFM data from a given dataset path and lenslet coordinates file.
    @param dataset_path - the path to the dataset
    @param lenslet_coords_file - the file containing the lenslet coordinates
    @param vol_shape - the shape of the volume
    @param img_shape - the shape of the images
    @param images_to_use - the images to use
    @param n_depths_to_fill - the number of depths to fill
    @param ds_id - the dataset ID
    @param volume_ths - the volume thresholds
    @param volume_quantiles - the volume quantiles [0,1.0]
    @param img_ths - the image thresholds
    @param norm - the normalization type: None, torch.max, torch.sum
    @return the processed dataset
    """
    with torch.no_grad():
        ds = XLFMDatasetFull(dataset_path, lenslet_coords_file, img_shape=img_shape,
                images_to_use=images_to_use,
                n_depths_to_fill=n_depths_to_fill,
                ds_id=ds_id)
        # Reshape volumes
        ds.vols = load_process_volume(ds.vols, vol_shape, volume_ths=volume_ths, norm=norm).float()
        ds.stacked_views = ds.stacked_views.float()

        # Quantile normalization
        if volume_quantiles[1] != 1:
            uper_ths = fast_quantile(ds.vols, volume_quantiles[1])
            ds.vols[ds.vols>uper_ths] = uper_ths
    
        img_low_ths = ds.stacked_views.max()*img_ths[0]
        ds.stacked_views[ds.stacked_views<img_low_ths] = 0
    
    return ds


def create_image_piramid(images, norm=np.max):
    """
    Create an image pyramid from a list of images. The first image is placed in the top left corner of the composite image. The remaining images are placed in a column to the right of the first image. Each subsequent image is placed below the previous image. The images are scaled to fit the composite image. The resulting composite image is returned.
    @param images - a list of images to be combined into a pyramid
    @param norm - a normalization function to be applied to the images
    @return a composite image of the image pyramid
    """
    rows, cols = images[0].shape
    rows2, cols2 = images[1].shape
    # Paint the borders black per image
    for img in images:
        border_color = img.max()
        img[0,:] = border_color
        img[-1,:] = border_color
        img[:,0] = border_color
        img[:,-1] = border_color
    torch2np = lambda i : i[0,...].permute(1,2,0).cpu().detach().numpy()
    composite_image = np.zeros((4*rows+rows2, 4*cols + cols2), dtype=np.float)
    # permute as input is b,c,x,y and we want x,y,c
    composite_image[:rows, :cols] = images[0]
    if norm is not None:
        composite_image[:rows, :cols] -= images[0].min()
        composite_image[:rows, :cols] /= norm(images[0])
    i_row = 0
    for ix,p in enumerate(images[1:]):
        n_rows, n_cols = p.shape

        if norm != None:
            p -= p.min()
            d = norm(p)
            if d==0:
                print(F'here')
                d = 1
            p = p/d
        else:
            p /= 2**(ix+1)
        
        composite_image[i_row:i_row + n_rows, cols:cols + n_cols] = (p)
        i_row += n_rows
    
    return composite_image[:i_row,:cols+cols2]


def set_all_seeds(seed):
    """
    Set the seed for all random number generators to ensure reproducibility.
    @param seed - the seed value to set
    @return None
    """
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    import random
    random.seed(seed)
    print(F'seed: {seed} set.')
    return

def volume_2_projections(vol_in, proj_type=torch.amax, scaling_factors=[1,1,2], depths_in_ch=False, ths=[0.0,1.0], normalize=False, border_thickness=2, add_scale_bars=False, scale_bar_vox_sizes=[40,20]):
    """
    This function takes a 3D volume and generates 2D projections of it along the x, y, and z axes.
    @param vol_in - the input volume
    @param proj_type - the type of projection to use (default is torch.amax)
    @param scaling_factors - the scaling factors to use for each axis (default is [1,1,2])
    @param depths_in_ch - whether the depth is in the channel dimension (default is False)
    @param ths - the thresholds to use for the volume (default is [0.0,1.0])
    @param normalize - whether to normalize the volume (default is False)
    @param border_thickness - the thickness of the border to add to the output image (default is 2)
    @param add
    """
    vol = vol_in.detach().clone().abs()
    # Normalize sets limits from 0 to 1
    if normalize:
        vol -= vol.min()
        vol /= vol.max()
    if depths_in_ch:
        vol = vol.permute(0,2,3,1).unsqueeze(1)
    if ths[0]!=0.0 or ths[1]!=1.0:
        vol_min,vol_max = vol.min(),vol.max()
        vol[(vol-vol_min)<(vol_max-vol_min)*ths[0]] = 0
        vol[(vol-vol_min)>(vol_max-vol_min)*ths[1]] = vol_min + (vol_max-vol_min)*ths[1]

    vol_size = list(vol.shape)
    vol_size[2:] = [vol.shape[i+2] * scaling_factors[i] for i in range(len(scaling_factors))]

    x_projection = proj_type(vol.float().cpu(), dim=2)
    y_projection = proj_type(vol.float().cpu(), dim=3)
    z_projection = proj_type(vol.float().cpu(), dim=4)

    out_img = z_projection.min() * torch.ones(
        vol_size[0], vol_size[1], vol_size[2] + vol_size[4] + border_thickness, vol_size[3] + vol_size[4] + border_thickness
    )

    out_img[:, :, : vol_size[2], : vol_size[3]] = z_projection
    out_img[:, :, vol_size[2] + border_thickness :, : vol_size[3]] = F.interpolate(x_projection.permute(0, 1, 3, 2), size=[vol_size[-1],vol_size[-3]], mode='nearest')
    out_img[:, :, : vol_size[2], vol_size[3] + border_thickness :] = F.interpolate(y_projection, size=[vol_size[2],vol_size[4]], mode='nearest')


    if add_scale_bars:
        line_color = 1.0
        # Draw white lines
        out_img[:, :, vol_size[2]: vol_size[2]+ border_thickness, ...] = line_color
        out_img[:, :, :, vol_size[3]:vol_size[3]+border_thickness, ...] = line_color

    return out_img


def imshow2D(img, blocking=False):
    """
    Display a 2D image using matplotlib.
    @param img - the image to display
    @param blocking - whether or not to block the program until the window is closed
    @return None
    """
    plt.figure(figsize=(10,10))
    plt.imshow(img[0,0,...].float().detach().cpu().numpy())
    if blocking:
        plt.show()

def imshow3D(vol, blocking=False, normalize=True, color_map='gray',add_scale_bars=False):
    """
    Display a 3D volume as a 2D projection image.
    @param vol - the 3D volume to display
    @param blocking - whether to block the program until the window is closed
    @param normalize - whether to normalize the volume
    @param color_map - the color map to use for the image
    @param add_scale_bars - whether to add scale bars to the image
    @return None
    """
    plt.figure(figsize=(10,10))
    plt.imshow(volume_2_projections(vol.permute(0,2,3,1).unsqueeze(1), normalize=normalize, add_scale_bars=add_scale_bars)[0,0,...].float().detach().cpu().numpy(), cmap=color_map)
    plt.axis('off')
    plt.tight_layout()
    if blocking:
        plt.show()


def save_image(tensor, path='output.png',color_map='gray'):
    """
    Save an image to a specified path. If the tensor is 3D, display it using imshow3D. If it is 2D, display it using imshow2D. If the path is a tif file, save the tensor as a tif file using imsave.
    @param tensor - the tensor to save
    @param path - the path to save the image to
    @param color_map - the color map to use when displaying the image
    @return None
    """
    if tensor.ndim<4:
        tensor = tensor.unsqueeze(0)
    if 'tif' in path:
        imsave(path, tensor[0,...].cpu().numpy().astype(np.float16))
        return
    if tensor.shape[1] == 1:
        imshow2D(tensor)
    else:
        imshow3D(tensor, color_map=color_map)
    plt.savefig(path, bbox_inches='tight',pad_inches = 0)


def psnr(img1, img2, PIXEL_MAX = 1.0):
    """
    Calculate the peak signal-to-noise ratio (PSNR) between two images.
    @param img1 - the first image
    @param img2 - the second image
    @param PIXEL_MAX - the maximum pixel value (default is 1.0)
    @return The PSNR value
    """
    mse = torch.mean( (img1 - img2) ** 2 )
    if mse == 0:
        if img1.sum()==0:
            return torch.tensor([0])
        return torch.tensor([100])
    
    return 20 * torch.log10(PIXEL_MAX / torch.sqrt(mse))

def composite_projection(tensor):
    """
    This function takes a 3D tensor and returns a composite projection of the tensor
    onto the xy, xz, and yz planes.
    @param tensor - a 3D tensor
    @return composite - a composite projection of the tensor onto the xy, xz, and yz planes.
    """
    # Compute the xy projection
    xy_projection = np.max(tensor, axis=0)
    # Compute the xz projection
    xz_projection = np.max(tensor, axis=1)
    # Compute the yz projection
    yz_projection = np.max(tensor, axis=2)
    # Transpose the yz projection to make its shape compatible with the top_half array
    yz_projection = np.transpose(yz_projection, (1, 0, 2))
    # Add padding to the yz projection
    yz_projection = np.pad(yz_projection, ((xz_projection.shape[0], 0), (0, 0), (0, 0)), mode='constant')
    # Stack the xy and xz projections vertically to create the first part of the composite image
    top_half = np.vstack((xy_projection, xz_projection))
    # Stack the yz projection and the first part horizontally to create the composite image
    composite = np.hstack((top_half, yz_projection))
    return composite

def filter_data(data, kernel_size=10):
    """
    Given a data array and a kernel size, filter the data using a moving average filter.
    @param data - the data array to be filtered
    @param kernel_size - the size of the kernel to be used in the moving average filter
    @return The filtered data array
    """
    kernel = np.ones(kernel_size) / kernel_size
    return np.convolve(data, kernel, mode='same')
    
def norm_data(data, filter=10):
    """
    Normalize the input data by subtracting the minimum value and dividing by the maximum value.
    @param data - the input data
    @param filter - the filter to apply to the data
    @return The normalized data and the range of the data
    """
    d1 = data * 1.0 # copy data
    if filter!=0:
        d1 = filter_data(d1, filter)
    min_d1 = min(d1)
    max_d1 = max(d1)
    d1 = d1-min_d1
    m_d1 = max_d1
    if m_d1 == 0:
        m_d1 = 1
    d2 = d1 / m_d1
    return d2,max_d1-min_d1,


############## Deconvolutions #########################

def roll_n(X, axis, n):
    """
    Roll the tensor X along the given axis by n positions.
    @param X - the tensor to roll
    @param axis - the axis to roll along
    @param n - the number of positions to roll
    @return The rolled tensor
    """
    f_idx = tuple(slice(None, None, None) if i != axis else slice(0, n, None) for i in range(X.dim()))
    b_idx = tuple(slice(None, None, None) if i != axis else slice(n, None, None) for i in range(X.dim()))
    front = X[f_idx]
    back = X[b_idx]
    return torch.cat([back, front], axis)
    
def batch_fftshift2d_real(x):
    """
    This function performs a 2D FFT shift on a batch of real-valued tensors.
    @param x - the input tensor
    @return The shifted tensor
    """
    out = x
    for dim in range(2, len(out.size())):
        n_shift = x.size(dim)//2
        if x.size(dim) % 2 != 0:
            n_shift += 1  # for odd-sized images
        out = roll_n(out, axis=dim, n=n_shift)
    return out  

# FFT convolution, the kernel fft can be precomputed
def fft_conv(A,B, fullSize, Bshape=[],B_precomputed=False):
    """
    This function performs a convolution between two tensors using the FFT algorithm.
    @param A - the first tensor
    @param B - the second tensor
    @param fullSize - the size of the output tensor
    @param Bshape - the shape of the second tensor
    @param B_precomputed - whether the second tensor has already been precomputed
    @return the result of the convolution and the precomputed second tensor
    """
    import torch.fft
    nDims = A.ndim-2
    padSizeA = (fullSize - torch.tensor(A.shape[2:]))
    padSizesA = torch.zeros(2*nDims,dtype=int)
    padSizesA[0::2] = torch.floor(padSizeA/2.0)
    padSizesA[1::2] = torch.ceil(padSizeA/2.0)
    padSizesA = list(padSizesA.numpy()[::-1])

    A_padded = F.pad(A,padSizesA)
    Afft = torch.fft.rfft2(A_padded)
    if B_precomputed:
        return batch_fftshift2d_real(torch.fft.irfft2( Afft * B.detach()))
    else:
        padSizeB = (fullSize - torch.tensor(B.shape[2:]))
        padSizesB = torch.zeros(2*nDims,dtype=int)
        padSizesB[0::2] = torch.floor(padSizeB/2.0)
        padSizesB[1::2] = torch.ceil(padSizeB/2.0)
        padSizesB = list(padSizesB.numpy()[::-1])
        B_padded = F.pad(B,padSizesB)
        Bfft = torch.fft.rfft2(B_padded)
        return batch_fftshift2d_real(torch.fft.irfft2( Afft * Bfft.detach())), Bfft.detach()

# Split an fft convolution into batches containing different depths
def fft_conv_split(A, B, psf_shape, n_split, B_precomputed=False, device = "cpu"):
    """
    This function performs a convolution between two tensors using FFT. The input tensors are split into smaller chunks to reduce memory usage. 
    @param A - the first tensor to be convolved
    @param B - the second tensor to be convolved
    @param psf_shape - the shape of the point spread function
    @param n_split - the number of splits to perform on the input tensors
    @param B_precomputed - a boolean indicating whether the second tensor has already been transformed using FFT
    @param device - the device to perform the computation on
    @return the result of the convolution
    """
    n_depths = A.shape[1]
    
    split_conv = n_depths//n_split
    depths = list(range(n_depths))
    depths = [depths[i:i + split_conv] for i in range(0, n_depths, split_conv)]

    fullSize = torch.tensor(A.shape[2:]) + psf_shape
    
    crop_pad = [int(psf_shape[i] - fullSize[i])//2 for i in range(0,2)]
    crop_pad = (crop_pad[1], (psf_shape[-1]- fullSize[-1])-crop_pad[1], crop_pad[0], (psf_shape[-2] - fullSize[-2])-crop_pad[0])
    # Crop convolved image to match size of PSF
    img_new = torch.zeros(A.shape[0], 1, psf_shape[0], psf_shape[1], device=device)
    if B_precomputed == False:
        OTF_out = torch.zeros(1, n_depths, fullSize[0], int(fullSize[1])//2+1, requires_grad=False, dtype=torch.complex64, device=device)
    for n in range(n_split):
        # print(n)
        curr_psf = B[:,depths[n],...].to(device)
        img_curr = fft_conv(A[:,depths[n],...].to(device), curr_psf, fullSize, psf_shape, B_precomputed)
        if B_precomputed == False:
            OTF_out[:,depths[n],...] = img_curr[1]
            img_curr = img_curr[0]
        img_curr = F.pad(img_curr, crop_pad)
        img_new += img_curr[:,:,:psf_shape[0],:psf_shape[1]].sum(1).unsqueeze(1).abs()
    
    if B_precomputed == False:
        return img_new, OTF_out
    return img_new


def load_PSF(filename, depths_to_use=[], interleaved=True):
    """
    Load a point spread function (PSF) from a file and return it as a tensor.
    @param filename - the name of the file containing the PSF
    @param depths_to_use - the depths to use in the PSF
    @param interleaved - whether the PSF is interleaved
    @return a tensor containing the PSF
    """
    # Load PSF
    print("Loading PSF...")
    try:
        # Check permute
        psfIn = torch.from_numpy(loadmat(filename)['PSF']).permute(2,0,1).unsqueeze(0)
    except:
        try:
            psfFile = h5py.File(filename,'r')
            psfIn = torch.from_numpy(psfFile.get('PSF')[:]).unsqueeze(0)
        except:
            psfIn = torch.from_numpy(imread(filename).astype(np.float32)).unsqueeze(0)

    # Make a square PSF
    psfIn = pad_img_to_min(psfIn)

    # Grab only needed depths
    if isinstance(depths_to_use, int):
        if depths_to_use==-1:
            depths_to_use = list(range(psfIn.shape[1]))
        else:
            n_depths = depths_to_use
            if interleaved:
                depths_to_use = torch.linspace(0, psfIn.shape[1], n_depths+2).long()[1:-1]
            else:
                depths_to_use = list(range(psfIn.shape[1]//2 - n_depths//2+1, psfIn.shape[1]//2- n_depths//2+1 + n_depths))
    psfIn = psfIn[:, depths_to_use, ...]
    # Normalize psfIn such that each depth sum is equal to 1
    for nD in range(psfIn.shape[1]):
        psfIn[:,nD,...] = psfIn[:,nD,...] / psfIn[:,nD,...].sum()
    
    return psfIn

def load_PSF_OTF(filename, vol_size, n_split=20, downS=1, device="cpu",
    dark_current=106, calc_max=False, compute_OTF=False):
    """
    Load the point spread function (PSF) and optical transfer function (OTF) from a file.
    @param filename - the name of the file containing the PSF
    @param vol_size - the size of the volume
    @param n_split - the number of splits
    @param downS - the downsample factor
    @param device - the device to use
    @param dark_current - the dark current
    @param calc_max - whether to calculate the maximum
    @param compute_OTF - whether to compute the OTF
    @return the OTF and the PSF shape
    """
                 
    n_depths = vol_size[-1]
    if n_split == -1:
        n_split = n_depths
    
    # Load PSF
    psfIn = load_PSF(filename, n_depths)

    psf_shape = torch.tensor(psfIn.shape[2:])
    vol = torch.rand(1,psfIn.shape[1], vol_size[0], vol_size[1], device=device)
    img, OTF = fft_conv_split(vol, psfIn.float().detach().to(device), psf_shape, n_split=n_split, device=device)
    
    OTF = OTF.detach()

    if compute_OTF:
        OTFt = torch.real(OTF) - 1j * torch.imag(OTF)
        OTF = torch.cat((OTF.unsqueeze(-1), OTFt.unsqueeze(-1)), 4)
    if calc_max:
        return OTF, psf_shape, psfMaxCoeffs
    else:
        return OTF,psf_shape


def XLFMDeconv(OTF, img, nIt, ObjSize=[512,512], PSFShape=[2160,2160], ROIsize=[512,512,90],\
                 errorMetric=F.mse_loss, n_split_fourier=1, update_median_limit_multiplier=10,\
                    max_allowed=4500, device='cuda:0', all_in_device=False, verbose=False):
    """
    This function performs deconvolution on an image using a given OTF. It returns the reconstructed object, the projections of the object, the estimated image, the losses, and the padding sizes.
    @param OTF - The optical transfer function
    @param img - The input image
    @param nIt - The number of iterations to perform
    @param ObjSize - The size of the object
    @param PSFShape - The shape of the point spread function
    @param ROIsize - The size of the region of interest
    @param errorMetric - The error metric to use
    @param n_split_fourier - The number of splits to use in the Fourier transform
    @param update_median_limit_multiplier - The multiplier to use when updating the median limit
    @param max_allowed - The maximum
    """
    
    if n_split_fourier == 1:
        n_split_fourier = OTF.shape[1]
    reconType = OTF.type()
    nDepths = OTF.shape[1]
    MI_projection_out = 0
    
    if img.sum()==0:
        volOut = torch.zeros(img.shape[0],nDepths, ObjSize[0], ObjSize[1]).type(reconType)
        MI_projection_out= volume_2_projections(volOut.permute(0,2,3,1).unsqueeze(1)).cpu()
        return volOut, MI_projection_out, img, []

    
    PSFShape = torch.tensor(PSFShape)

    # Compute transposed OTF
    if OTF.ndim==4: # meaning that the transpose hasn't been computed
        OTF = torch.cat((OTF.unsqueeze(-1), (torch.real(OTF) - 1j * torch.imag(OTF)).unsqueeze(-1)),dim=4)
    OTFt = OTF[...,1].clone()
    OTF = OTF[...,0].clone()

    padSize = 2*[(OTF.shape[2] - ObjSize[0])//2] + 2*[(OTF.shape[2] - ObjSize[1])//2]
    padSizeImg = 2*[(OTF.shape[2] - img.shape[2])//2] + 2*[(OTF.shape[2] - img.shape[3])//2]

    # Pad input
    ImgExp = F.pad(img, padSizeImg).to(device)

    with torch.no_grad():
        # Initialize reconstructed volume
        ObjRecon = torch.ones(1,nDepths,ObjSize[0],ObjSize[1])#.type(dtype=reconType)
        
        ImgEst = 0* ImgExp.clone()

        if all_in_device:
            ObjRecon = ObjRecon.to(device)
            OTF = OTF.to(device)
            OTFt = OTFt.to(device)
            ImgEst = ImgEst.to(device)
            ImgExp = ImgExp.to(device)

        losses = []
        end = "\r"
        # plt.ion()
        # plt.figure()
        for ii in tqdm(range(nIt), position=1, desc="Deconv Iter.", leave=False, colour='red'):
            
            # Compute current image estimate (forward projection)
            ImgEst *= 0.0
            ObjTemp = F.pad(ObjRecon, padSize)
            for jj in range(0,nDepths, n_split_fourier):
                curr_depths = list(range(jj, min(jj+n_split_fourier, nDepths)))
                planeOTF = OTF[:,curr_depths,...].to(device)
                currObjPlanes = ObjTemp[:,curr_depths,...]
                planeObjFFT = torch.fft.rfft2(currObjPlanes).to(device)
                ImgEst += F.relu(batch_fftshift2d_real(torch.fft.irfft2(planeObjFFT * planeOTF))).sum(1).unsqueeze(1)
            Tmp = ImgExp / (ImgEst+1e-8)    
            if Tmp[Tmp!=0].numel()>0:
                Tmp.clamp_(0.0,Tmp[Tmp!=0].median()*update_median_limit_multiplier)
            
            Ratio = Tmp.to(device)

            if torch.isnan(Ratio).any():
                print(F'nan found at it: {ii+1} ')
                break
            # Propagate error back to volume space and update volume
            for jj in range(0,nDepths, n_split_fourier):
                curr_depths = list(range(jj,min(jj+n_split_fourier, nDepths)))
                planeObj = ObjTemp[:,curr_depths,...].to(device)
                planeOTF = OTFt[:,curr_depths,...].to(device)
                ObjRecon[:,curr_depths,...] = F.pad(planeObj * batch_fftshift2d_real(torch.fft.irfft2(torch.fft.rfft2(Ratio) * planeOTF)),[-p for p in padSize]).type(ObjRecon.type())
            
            if verbose:
                MI_projection_out= volume_2_projections(ObjRecon, depths_in_ch=True).cpu()

                curr_error = errorMetric(ImgExp,ImgEst).item()
                losses.append(curr_error)
                if ii==nIt-1:
                    end = "\n"
                max_value =ObjRecon.float().max()
                print(F'Deconv it: {ii+1} / {nIt} \t currErr: {curr_error} \t maxVal: {max_value}', end=end)
                ## Uncomment to show some images
                plt.subplot(1,3,1)
                plt.imshow(ImgExp[0,0,...].cpu().numpy())
                plt.subplot(1,3,2)
                plt.imshow(ImgEst[0,0,...].cpu().numpy())
                plt.subplot(1,3,3)
                plt.imshow(MI_projection_out[0,0,...].cpu().numpy())
                plt.pause(0.1)
                plt.show()
        
    ObjRecon[:,0:nDepths//2-ROIsize[2]//2,...] = 0
    ObjRecon[:,nDepths//2+ROIsize[2]//2:,...] = 0
    return ObjRecon,MI_projection_out,ImgEst,losses, padSize, padSizeImg
